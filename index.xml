<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Haozhe Su on Haozhe Su</title>
    <link>https://SoldierDown.github.io/</link>
    <description>Recent content in Haozhe Su on Haozhe Su</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Haozhe Su</copyright>
    <lastBuildDate>Sat, 06 Oct 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Code for Weakly-Supervised GANs (WS-GAN)</title>
      <link>https://SoldierDown.github.io/post/ws-gan/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 -0500</pubDate>
      
      <guid>https://SoldierDown.github.io/post/ws-gan/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Checkout the code&lt;/strong&gt; on &lt;a href=&#34;https://github.com/phymhan/ws-gan&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Github&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;PyTorch code for &amp;ldquo;Learning Facial Attribute Modeling with Weakly Supervised GANs&amp;rdquo;. The code is largely based on &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#34; target=&#34;_blank&#34;&gt;CycleGAN and pix2pix&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Facial Attribute Modeling with Weakly Supervised GANs</title>
      <link>https://SoldierDown.github.io/publication/ws-gan/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/publication/ws-gan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Learning Generative Models of Tissue Organization with Supervised GANs</title>
      <link>https://SoldierDown.github.io/publication/supervised-gan/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/publication/supervised-gan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Contextual Recurrent Residual Networks for Scene Labeling</title>
      <link>https://SoldierDown.github.io/publication/deep_contextual_rnn/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/publication/deep_contextual_rnn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Study of J/Psi-&gt;gamma&#43;Ks&#43;Ks&#43;pi</title>
      <link>https://SoldierDown.github.io/project/physics_urp/</link>
      <pubDate>Fri, 15 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/physics_urp/</guid>
      <description>&lt;p&gt;In Highe Energy Physics(HEP), mu(1405) and mu(1475) both can decay into K+K_bar+pi as suggested by the Mark III analysis. However, BESII analysis suggests that if an energy-dependent width is applied, it is not neccessary to have two states in J/Psi-&amp;gt;K+K_bar+pi.&lt;/p&gt;

&lt;p&gt;The motivation of this project is to study n(1405) and mu(1475) using high-statistics J/Psi samples at BESIII, which is composed the following steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Implement event selection, background estimations and signal fitting.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Made comparisons between data and M-C simulation results.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;a href=&#34;https://SoldierDown.github.io/pdf/physics_urp.pdf&#34;&gt;[&lt;strong&gt;View my presentation&lt;/strong&gt;]&lt;/a&gt;
</description>
    </item>
    
    <item>
      <title>Code for Supervised GANs (SGAN)</title>
      <link>https://SoldierDown.github.io/post/supervised-gan/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 -0500</pubDate>
      
      <guid>https://SoldierDown.github.io/post/supervised-gan/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Checkout the code&lt;/strong&gt; on &lt;a href=&#34;https://github.com/phymhan/supervised-gan&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Github&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;PyTorch code for &amp;ldquo;Learning Generative Models of Tissue Organization with Supervised GANs&amp;rdquo;. The code is largely based on &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#34; target=&#34;_blank&#34;&gt;CycleGAN and pix2pix&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Automated Blood Cell Detection and Segmentation System</title>
      <link>https://SoldierDown.github.io/publication/automatic-cell/</link>
      <pubDate>Mon, 13 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/publication/automatic-cell/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MatConvNet modules for 2-D RNNs (LSTM, GRU)</title>
      <link>https://SoldierDown.github.io/post/lstm2d/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 -0500</pubDate>
      
      <guid>https://SoldierDown.github.io/post/lstm2d/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Checkout the code&lt;/strong&gt; on &lt;a href=&#34;https://github.com/phymhan/matconvnet&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Github&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Implementation of various &lt;a href=&#34;https://arxiv.org/abs/1509.00552&#34; target=&#34;_blank&#34;&gt;2-D RNN&lt;/a&gt; modules is included:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2-D RNN&lt;/li&gt;
&lt;li&gt;2-D &lt;a href=&#34;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34; target=&#34;_blank&#34;&gt;LSTM&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Vanilla&lt;/li&gt;
&lt;li&gt;With &amp;ldquo;&lt;a href=&#34;ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf&#34; target=&#34;_blank&#34;&gt;peephole connections&lt;/a&gt;&amp;ldquo;&lt;/li&gt;
&lt;li&gt;With &lt;em&gt;coupled&lt;/em&gt; forget and input gates&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2-D &lt;a href=&#34;http://arxiv.org/pdf/1406.1078v3.pdf&#34; target=&#34;_blank&#34;&gt;GRU&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hierarchical Feature Extraction for Nuclear Morphometry-Based Cancer Diagnosis</title>
      <link>https://SoldierDown.github.io/publication/hierarchical/</link>
      <pubDate>Fri, 21 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/publication/hierarchical/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2D Interactive Smoke Simulation</title>
      <link>https://SoldierDown.github.io/project/2d_fluid/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/2d_fluid/</guid>
      <description>&lt;p&gt;Smoke simulation is one of the most important parts in physics-based simulation. In this project, our goal is to visualize 2D smoke based on 2D Navier-Stokes equation. It is composed of the following steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First of all, we apply numerical methods and solve Navier-Stokes equation by splitting the original equation into three major parts: advection, diffusion and projection. Through advection, the properties of smoke move along with the velocity field. Through diffusion, density tends to be uniform in the domain. And the final part, projection is to make the velocity field divergence-free, which means the smoke is incompressible.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Secondly, we design a GUI using QT. This GUI helps us interact with the smoke. We can add source to the domain simply using mouse, pause/restart the simulation and run the simulation frame by frame.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;a href=&#34;https://SoldierDown.github.io/pdf/Term_Project.pdf&#34;&gt;[&lt;strong&gt;View our technical report&lt;/strong&gt;]&lt;/a&gt;
</description>
    </item>
    
    <item>
      <title>3D Animation for Education Purpose</title>
      <link>https://SoldierDown.github.io/project/educational/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/educational/</guid>
      <description>&lt;p&gt;Convolutional neural networks, or CNNs, has been widely applied to various facial related problems, including facial land-marking, individual face matching, and 3D facial reconstruction. Despite its success in robust face detection in terms of performance, there are still some challenges raised by strong illumination, sizes of faces and artifacts. Particularly, in CNN training, the feature extraction and detection results are greatly affected by the structure of the network. Hence, it is desirable to be able to perform some kind of analysis on convolution neural nets, such as feature map visualization at different stages, which helps design of network structure and tuning of hyper-parameters and thus improves feature extraction as well as classification accuracy.&lt;/p&gt;

&lt;p&gt;Our project aims to propose and evaluate different visualization strategies for face detection models, which is composed of two steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the first step, we train a face detection model. We first transform the state-of-art Faster-RCNN on object detection to the face detection problem. The modified RCNN model is then tested on challenging face images, including the Wider Face dataset and the pre-trained model for CNN is the VGG-16. For implementation we use Caffe as the CNN framework.&lt;/li&gt;
&lt;li&gt;In the second step we evaluate and visualize our model using the proposed classifier probes, which is based on computing the conditional entropy. Basically, the (average) conditional entropy of class given activation values quantifies the average amount of information of classes needed when activation values are known. Ideally, from bottom to top (from bottom image layer to top convolutional layer), we would expect the average conditional entropy goes down.&lt;/li&gt;
&lt;/ul&gt;

&lt;a href=&#34;https://SoldierDown.github.io/pdf/Term_Project.pdf&#34;&gt;[&lt;strong&gt;View our technical report&lt;/strong&gt;]&lt;/a&gt;
</description>
    </item>
    
    <item>
      <title>3D Modeling: Bear-Claw Knife, Elevator and Other Stuffs</title>
      <link>https://SoldierDown.github.io/project/3danse_cp/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/3danse_cp/</guid>
      <description>&lt;p&gt;Convolutional neural networks, or CNNs, has been widely applied to various facial related problems, including facial land-marking, individual face matching, and 3D facial reconstruction. Despite its success in robust face detection in terms of performance, there are still some challenges raised by strong illumination, sizes of faces and artifacts. Particularly, in CNN training, the feature extraction and detection results are greatly affected by the structure of the network. Hence, it is desirable to be able to perform some kind of analysis on convolution neural nets, such as feature map visualization at different stages, which helps design of network structure and tuning of hyper-parameters and thus improves feature extraction as well as classification accuracy.&lt;/p&gt;

&lt;p&gt;Our project aims to propose and evaluate different visualization strategies for face detection models, which is composed of two steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the first step, we train a face detection model. We first transform the state-of-art Faster-RCNN on object detection to the face detection problem. The modified RCNN model is then tested on challenging face images, including the Wider Face dataset and the pre-trained model for CNN is the VGG-16. For implementation we use Caffe as the CNN framework.&lt;/li&gt;
&lt;li&gt;In the second step we evaluate and visualize our model using the proposed classifier probes, which is based on computing the conditional entropy. Basically, the (average) conditional entropy of class given activation values quantifies the average amount of information of classes needed when activation values are known. Ideally, from bottom to top (from bottom image layer to top convolutional layer), we would expect the average conditional entropy goes down.&lt;/li&gt;
&lt;/ul&gt;

&lt;a href=&#34;https://SoldierDown.github.io/pdf/Maya.pdf&#34;&gt;[&lt;strong&gt;View our technical report&lt;/strong&gt;]&lt;/a&gt;
</description>
    </item>
    
    <item>
      <title>Image Warping</title>
      <link>https://SoldierDown.github.io/project/cg_iw_cp/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/cg_iw_cp/</guid>
      <description>&lt;p&gt;Convolutional neural networks, or CNNs, has been widely applied to various facial related problems, including facial land-marking, individual face matching, and 3D facial reconstruction. Despite its success in robust face detection in terms of performance, there are still some challenges raised by strong illumination, sizes of faces and artifacts. Particularly, in CNN training, the feature extraction and detection results are greatly affected by the structure of the network. Hence, it is desirable to be able to perform some kind of analysis on convolution neural nets, such as feature map visualization at different stages, which helps design of network structure and tuning of hyper-parameters and thus improves feature extraction as well as classification accuracy.&lt;/p&gt;

&lt;p&gt;Our project aims to propose and evaluate different visualization strategies for face detection models, which is composed of two steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the first step, we train a face detection model. We first transform the state-of-art Faster-RCNN on object detection to the face detection problem. The modified RCNN model is then tested on challenging face images, including the Wider Face dataset and the pre-trained model for CNN is the VGG-16. For implementation we use Caffe as the CNN framework.&lt;/li&gt;
&lt;li&gt;In the second step we evaluate and visualize our model using the proposed classifier probes, which is based on computing the conditional entropy. Basically, the (average) conditional entropy of class given activation values quantifies the average amount of information of classes needed when activation values are known. Ideally, from bottom to top (from bottom image layer to top convolutional layer), we would expect the average conditional entropy goes down.&lt;/li&gt;
&lt;/ul&gt;

&lt;a href=&#34;https://SoldierDown.github.io/pdf/Term_Project.pdf&#34;&gt;[&lt;strong&gt;View our technical report&lt;/strong&gt;]&lt;/a&gt;
</description>
    </item>
    
    <item>
      <title>Rigid Body Simulation</title>
      <link>https://SoldierDown.github.io/project/rbs/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/rbs/</guid>
      <description>&lt;p&gt;Convolutional neural networks, or CNNs, has been widely applied to various facial related problems, including facial land-marking, individual face matching, and 3D facial reconstruction. Despite its success in robust face detection in terms of performance, there are still some challenges raised by strong illumination, sizes of faces and artifacts. Particularly, in CNN training, the feature extraction and detection results are greatly affected by the structure of the network. Hence, it is desirable to be able to perform some kind of analysis on convolution neural nets, such as feature map visualization at different stages, which helps design of network structure and tuning of hyper-parameters and thus improves feature extraction as well as classification accuracy.&lt;/p&gt;

&lt;p&gt;Our project aims to propose and evaluate different visualization strategies for face detection models, which is composed of two steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the first step, we train a face detection model. We first transform the state-of-art Faster-RCNN on object detection to the face detection problem. The modified RCNN model is then tested on challenging face images, including the Wider Face dataset and the pre-trained model for CNN is the VGG-16. For implementation we use Caffe as the CNN framework.&lt;/li&gt;
&lt;li&gt;In the second step we evaluate and visualize our model using the proposed classifier probes, which is based on computing the conditional entropy. Basically, the (average) conditional entropy of class given activation values quantifies the average amount of information of classes needed when activation values are known. Ideally, from bottom to top (from bottom image layer to top convolutional layer), we would expect the average conditional entropy goes down.&lt;/li&gt;
&lt;/ul&gt;

&lt;a href=&#34;https://SoldierDown.github.io/pdf/&#34;&gt;[&lt;strong&gt;View our technical report&lt;/strong&gt;]&lt;/a&gt;
</description>
    </item>
    
    <item>
      <title>This is Us</title>
      <link>https://SoldierDown.github.io/project/this_is_us/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/this_is_us/</guid>
      <description>&lt;p&gt;Convolutional neural networks, or CNNs, has been widely applied to various facial related problems, including facial land-marking, individual face matching, and 3D facial reconstruction. Despite its success in robust face detection in terms of performance, there are still some challenges raised by strong illumination, sizes of faces and artifacts. Particularly, in CNN training, the feature extraction and detection results are greatly affected by the structure of the network. Hence, it is desirable to be able to perform some kind of analysis on convolution neural nets, such as feature map visualization at different stages, which helps design of network structure and tuning of hyper-parameters and thus improves feature extraction as well as classification accuracy.&lt;/p&gt;

&lt;p&gt;Our project aims to propose and evaluate different visualization strategies for face detection models, which is composed of two steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the first step, we train a face detection model. We first transform the state-of-art Faster-RCNN on object detection to the face detection problem. The modified RCNN model is then tested on challenging face images, including the Wider Face dataset and the pre-trained model for CNN is the VGG-16. For implementation we use Caffe as the CNN framework.&lt;/li&gt;
&lt;li&gt;In the second step we evaluate and visualize our model using the proposed classifier probes, which is based on computing the conditional entropy. Basically, the (average) conditional entropy of class given activation values quantifies the average amount of information of classes needed when activation values are known. Ideally, from bottom to top (from bottom image layer to top convolutional layer), we would expect the average conditional entropy goes down.&lt;/li&gt;
&lt;/ul&gt;

&lt;a href=&#34;https://SoldierDown.github.io/pdf/Term_Project.pdf&#34;&gt;[&lt;strong&gt;View our technical report&lt;/strong&gt;]&lt;/a&gt;
</description>
    </item>
    
  </channel>
</rss>
