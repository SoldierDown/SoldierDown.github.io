<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Haozhe Su on Haozhe Su</title>
    <link>https://SoldierDown.github.io/</link>
    <description>Recent content in Haozhe Su on Haozhe Su</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Haozhe Su</copyright>
    <lastBuildDate>Sat, 06 Oct 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Rigid Body Simulation</title>
      <link>https://SoldierDown.github.io/project/rbs/</link>
      <pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/rbs/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Study of J/Psi-&gt;gamma&#43;Ks&#43;Ks&#43;pi</title>
      <link>https://SoldierDown.github.io/project/physics_urp/</link>
      <pubDate>Fri, 15 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/physics_urp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2D Interactive Smoke Simulation</title>
      <link>https://SoldierDown.github.io/project/2d_fluid/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/2d_fluid/</guid>
      <description></description>
    </item>
    
    <item>
      <title>3D Animation for Educational Purpose</title>
      <link>https://SoldierDown.github.io/project/educational/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/educational/</guid>
      <description></description>
    </item>
    
    <item>
      <title>3D Modeling: Bear-Claw Knife, Elevator and Other Stuffs</title>
      <link>https://SoldierDown.github.io/project/3danse_cp/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/3danse_cp/</guid>
      <description>&lt;p&gt;Convolutional neural networks, or CNNs, has been widely applied to various facial related problems, including facial land-marking, individual face matching, and 3D facial reconstruction. Despite its success in robust face detection in terms of performance, there are still some challenges raised by strong illumination, sizes of faces and artifacts. Particularly, in CNN training, the feature extraction and detection results are greatly affected by the structure of the network. Hence, it is desirable to be able to perform some kind of analysis on convolution neural nets, such as feature map visualization at different stages, which helps design of network structure and tuning of hyper-parameters and thus improves feature extraction as well as classification accuracy.&lt;/p&gt;

&lt;p&gt;Our project aims to propose and evaluate different visualization strategies for face detection models, which is composed of two steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the first step, we train a face detection model. We first transform the state-of-art Faster-RCNN on object detection to the face detection problem. The modified RCNN model is then tested on challenging face images, including the Wider Face dataset and the pre-trained model for CNN is the VGG-16. For implementation we use Caffe as the CNN framework.&lt;/li&gt;
&lt;li&gt;In the second step we evaluate and visualize our model using the proposed classifier probes, which is based on computing the conditional entropy. Basically, the (average) conditional entropy of class given activation values quantifies the average amount of information of classes needed when activation values are known. Ideally, from bottom to top (from bottom image layer to top convolutional layer), we would expect the average conditional entropy goes down.&lt;/li&gt;
&lt;/ul&gt;

&lt;a href=&#34;https://SoldierDown.github.io/pdf/Maya.pdf&#34;&gt;[&lt;strong&gt;View our technical report&lt;/strong&gt;]&lt;/a&gt;
</description>
    </item>
    
    <item>
      <title>This is Us</title>
      <link>https://SoldierDown.github.io/project/this_is_us/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/this_is_us/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Visualization of Signed Distance Function</title>
      <link>https://SoldierDown.github.io/project/sdf/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/sdf/</guid>
      <description>&lt;p&gt;Convolutional neural networks, or CNNs, has been widely applied to various facial related problems, including facial land-marking, individual face matching, and 3D facial reconstruction. Despite its success in robust face detection in terms of performance, there are still some challenges raised by strong illumination, sizes of faces and artifacts. Particularly, in CNN training, the feature extraction and detection results are greatly affected by the structure of the network. Hence, it is desirable to be able to perform some kind of analysis on convolution neural nets, such as feature map visualization at different stages, which helps design of network structure and tuning of hyper-parameters and thus improves feature extraction as well as classification accuracy.&lt;/p&gt;

&lt;p&gt;Our project aims to propose and evaluate different visualization strategies for face detection models, which is composed of two steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;In the first step, we train a face detection model. We first transform the state-of-art Faster-RCNN on object detection to the face detection problem. The modified RCNN model is then tested on challenging face images, including the Wider Face dataset and the pre-trained model for CNN is the VGG-16. For implementation we use Caffe as the CNN framework.&lt;/li&gt;
&lt;li&gt;In the second step we evaluate and visualize our model using the proposed classifier probes, which is based on computing the conditional entropy. Basically, the (average) conditional entropy of class given activation values quantifies the average amount of information of classes needed when activation values are known. Ideally, from bottom to top (from bottom image layer to top convolutional layer), we would expect the average conditional entropy goes down.&lt;/li&gt;
&lt;/ul&gt;

&lt;a href=&#34;https://SoldierDown.github.io/pdf/Term_Project.pdf&#34;&gt;[&lt;strong&gt;View our technical report&lt;/strong&gt;]&lt;/a&gt;
</description>
    </item>
    
    <item>
      <title>Digital Art Using Processing</title>
      <link>https://SoldierDown.github.io/project/processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://SoldierDown.github.io/project/processing/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
